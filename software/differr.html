<!DOCTYPE html>
<html>
  <head>
    <title> differr: error propagation through automatic differentiation 
    </title>
    <link rel="icon" type=images/icon.jpg" href="../images/icon.jpg">
    <meta charset="UTF-8">
    <meta name="description" content="Jean Tedros website">
    <meta name="author" content="Jean Tedros">
    <meta name="viewport" content="width=device-width, initial-scale=0.7">
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async 
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <link rel="stylesheet" href="../style.css">
  </head>
  <body style="max-width: 500px; margin: auto; color: rgb(0,0,0);
  background-color: rgb(255,235,225); font-size: 12px">
      <br>
      <b>differr: error propagation through automatic differentiation</b><br>
      <i>How to propagate uncertainties through a chain of non-linear 
      operations ?</i><br><br>
      
      <div>
        <p style="font-size:11px">
        <b>Idea</b><br>
        </p>
        Let \(\boldsymbol{x} \in \mathbb{R}^{n}\) be a random vector with mean
        \(\boldsymbol{\mu_{x}}\) and covariance matrix
        \(\boldsymbol{\Sigma_{x}} \) and let \(f: \mathbb{R}^{n} \rightarrow
        \mathbb{R}^{m}\) be a nonlinear function. Up to first-order
        approximation, \(\boldsymbol{y}= f(\boldsymbol{x}) \approx
        f(\boldsymbol{\mu_{x}}) + J(\boldsymbol{x- \mu_{x}})\), where \(J \in
        \mathbb{R}^{m \times n}\) is the Jacobian matrix \(\partial f/\partial
        \boldsymbol{x}\) evaluated at \(\boldsymbol{\mu_{x}}\). This
        approximation is reasonably accepted\(^{*}\) and the random vector
        \(\boldsymbol{y} \in \mathbb{R}^{m}\) has mean \(\boldsymbol{\mu_{y}}
        \approx f(\boldsymbol{\mu_{x}})\) and covariance \(\Sigma_{y} \approx J
        \Sigma_{x} J^{T}\).<br><br> For example, if \(\boldsymbol{x}\) is
        composed of two random vectors \(\boldsymbol{a}\) and
        \(\boldsymbol{b}\) such that \(\boldsymbol{x}=( \boldsymbol{a},
        \boldsymbol{b})\), then<br>
        <p style="text-align:center">    
            \(\Sigma_{y} = \begin{bmatrix} J_{\boldsymbol{a}} & 
            J_{\boldsymbol{b}} \end{bmatrix} \begin{bmatrix}
            \sigma_{\boldsymbol{a}} & \sigma_{\boldsymbol{ab}} \\ 
            \sigma_{\boldsymbol{ba}} & \sigma_{\boldsymbol{b}} 
            \end{bmatrix} \begin{bmatrix} J_{\boldsymbol{a}}^{T} \\
            J_\boldsymbol{b}^{T} \end{bmatrix} + O(N^2) \) 
            &nbsp;&nbsp;&nbsp;&nbsp;(1)
        </p>
        <br>
        where \(J_{\boldsymbol{a}}=\partial\boldsymbol{y}/\partial
        \boldsymbol{a}\) and \(J_{\boldsymbol{b}}=\partial\boldsymbol{y}/
        \partial \boldsymbol{b} \).<br><br>
        
        It can be seen that the uncertainty, characterized by the covariance 
        matrix, \( \Sigma_{\boldsymbol{x}} \), of the input data 
        \(\boldsymbol{x}\), is propagated to first degree by the Jacobian, 
        \(J\), through the operation, \( f \).<br><br> 
        
        <p style="font-size:11px">
        <b>Method</b><br>
        </p>
        Automatic differentiation relies on the fact that every operation, as 
        complex as it gets, boils down to a sequence of elementary arithmetic 
        operations and functions. And therefore, by tracking the chain of 
        operations for every input data from beginning to end, one could in 
        principle apply the chain rule repeatedly and compute partial 
        derivatives automatically.<br><br>

        <p style="font-size:11px">
        <b>Implementation</b><br>
        </p>
        Write about software ..<br><br>

        <p style="font-size:11px">
        <b>Example</b><br>
        </p>
        We'll illustrate our idea with an application from multi-wavelength
        astronomy, where we are interested in comparing images of the same
        physical space taken from different instruments with different
        resolutions. One would need to degrade the images to a minimum common
        resolution and re-project to a common grid; by this, changing the pixel
        size, orientation, while preserving the colors of the astronomical
        source. This task involves applying a chain of convolution operations
        in the pixel space with specially constructed non-Gaussian kernels\(^{**
        }\) depending on the ratio of the final and initial pixel size and
        resolution.<br><br>
        
        Let \( \boldsymbol{x} \in \mathbb{R}^{n\times m} \), be a regularly
        gridded image of dimensions \( n\times m \) with covariance matrix \(
        \Sigma_{\boldsymbol{x}} \). And let, \( y \in \mathbb{R}^{n'\times m'}
        \), be the result image of dimensions \( n'\times m' \) with \( n' \leq
        n \) and \( m' \leq m \), where \( \boldsymbol{y} = h( \boldsymbol{x})
        = ( g \circ f)(\boldsymbol{x}) \), with \(f\) and \(g\) the degrading
        and re-gridding operation respectively.<br><br>
        <table style="text-align:left;background-color:rgb(255, 221,170); 
        width:100%" border="4" cellpadding="10" cellspacing="2">
        <tbody>
            <tr> 
            <td> 
            <img
                src="../images/NGC4254_PACS1_SPIRE1.png"
                alt="Result of degrading and re-gridding NGC4254"
                width="100%"
                title="Result of degrading and re-gridding NGC4254">
                <p style="color:rgb(51,51,153);text-align:center"> 
                <i>On the left: the original \(90\times 90\) image of NGC4254 as
                seen by the instrument PACS on-board of the Herschel space
                telescope at a wavelength of 70 \(\mu\)m and pixel size of 2
                arcseconds. On the right: degrading and re-gridding to the
                resolution of the SPIRE instrument on-board of the Herschel
                space telescope at a wavelength of 250 \(\mu \)m and pixel size
                of 6 arcseconds resulting in a \( 33 \times 33\) image.</i>
                </p>
            </td>
            </tr>
        </tbody>
        </table>
        <br>
        The Jacobian of \(h\) at \( \boldsymbol {x}\), \( J_{h}(\boldsymbol{x})
        \in \mathbb{R}^{n'\times m' \times n \times m}\), where every element
        \(J_{i'j'}\) with \(1\leq i'\leq n'\) and \(1\leq j'\leq m'\), is an
        \(n\times m\) matrix containing the first derivatives of \(
        \boldsymbol{y} \) with respect to the pixel \(\boldsymbol{x}_{ij} \)
        with \(1\leq i\leq n \) and \(1\leq j\leq m\). In total, the Jacobian
        will contain \(n'\times m' \times n \times m\) first derivatives of the
        every pixel of the final image, \(y_{i'j'}\), with respect to the input
        image, \(\boldsymbol{x}\).<br><br>
        <table style="text-align:left;background-color:rgb(255, 221,170); 
        width:100%" border="4" cellpadding="10" cellspacing="2">
        <tbody>
            <tr> 
            <td> 
            <img
                src="../images/jacobian_NGC4254.png"
                alt="Jacobian of degrading and re-gridding NGC4254"
                width="100%"
                title="Jacobian of degrading and re-gridding NGC4254">
                <p style="color:rgb(51,51,153);text-align:center"> 
                <i>Visualization of the Jacobian, \(J_{h}(\boldsymbol{x})\).
                Every element, \(J_{i'j'}\), shows the derivative of \( 
                y_{i'j'}\), with respect to \(\boldsymbol{x}\). Therefore
                showing the dependence of every pixel of \(\boldsymbol{y}\) 
                on \(\boldsymbol{x}\).</i>
                </p>
            </td>
            </tr>
        </tbody>
        </table>
        <br> 
        As stated above, the Jacobian, \(J_{h}(\boldsymbol{x})\in
        \mathbb{R}^{n'\times m'\times n\times m}\), is a \(n'\times m'\) matrix
        (here \(33\times 33\)) where every element is a \(n\times m\) matrix
        (here \(90\times 90\)) containing the derivative of \(y_{i'j'}\) with
        respect to \(\boldsymbol{x}\). Every pixel \(J_{i'j'}\) shows the
        dependence of \(y_{i'j'}\) to \(\boldsymbol{x}\).  In order to estimate
        the uncertainty of the final image, \( \Sigma_{\boldsymbol{y}} \), one
        would then have to make use of equation \((1)\) to propagate the
        uncertainty, \( \Sigma_{\boldsymbol{ x}} \), of the initial
        image.<br><br>
        <table style="text-align:left;background-color:rgb(255, 221,170); 
        width:100%" border="4" cellpadding="10" cellspacing="2">
        <tbody>
            <tr> 
            <td> 
            <img
                src="../images/propagated_NGC4254.png"
                alt="Propagated uncertainty for NGC4254"
                width="100%"
                title="Propagated uncertainty for NGC4254">
                <p style="color:rgb(51,51,153);text-align:center"> 
                <i>On the left: the uncertainty on the pixels of the initial
                image \( \boldsymbol{x} \). On the right: the propagated 
                uncertainty of \(\boldsymbol{y}\).</i>
                </p>
            </td>
            </tr>
        </tbody>
        </table>
        <br>
        <p style="font-size:11px">
        </p>
        There exists other techniques, often used in astronomy, to estimate the
        propagated uncertainty through complex non-linear operations. It is
        possible to derive analytical solutions for the propagated uncertainty,
        under various statistical choices and approximations. Klein
        (2021)\(^{***}\) presented an analytical approximation to estimate 
        the propagated uncertainty when convolving an image with a Gaussian 
        kernel for both correlated and uncorrelated data. The result states that
        the propagated uncertainty of uncorrelated data, i.e where \(
        \Sigma_{\boldsymbol{x}}\) is diagonal, is calculated by convolving the 
        initial uncertainty with the square of the kernel.<br>
        <p style="text-align:center">    
            \(\Sigma_{\boldsymbol{y}} = \Sigma_{\boldsymbol{x}}\star K^{2}\)  
        </p>
        Where \(\star\) is the convolution operator and \(K\) is 
        a 2D normalized Gaussian kernel. In the following, we compare this 
        method to the <i>differr</i> method applied to the resolution 
        degradation step for the same example as before. We find that the two 
        methods yield similar results up to 0.1% in mean absolute percentage 
        error over the whole image.<br><br>
        <table style="text-align:left;background-color:rgb(255, 221,170); 
        width:100%" border="4" cellpadding="10" cellspacing="2">
        <tbody>
            <tr> 
            <td> 
            <img
                src="../images/differr_klein.png"
                alt="Propagated uncertainty for NGC4254"
                width="100%"
                title="Propagated uncertainty for NGC4254">
                <p style="color:rgb(51,51,153);text-align:center"> 
                <i>On the left: the propagated uncertainty as estimated by
                the differr method. On the right: the propagated 
                uncertainty as estimated by the method explained in 
                Klein (2021).</i>
                </p>
            </td>
            </tr>
        </tbody>
        </table>
        <br>
        A more applicable method, is to estimate the propagated uncertainty by
        repeating the same process and by adding a random noise to each pixel
        from a Gaussian distribution with a standard deviation equal to the
        uncertainty associated with each pixel. With enough repetition, one can
        estimate the propagated uncertainty of each output pixel by the
        standard deviation of its values over all iterations. This method,
        known as bootstrapping or Monte-Carlo, can be shown to be equivalent to
        equation \((1)\), and therefore converges after enough iterations to
        the <i>differr</i> method.<br><br>
        <table style="text-align:left;background-color:rgb(255, 221,170); 
        width:100%" border="4" cellpadding="10" cellspacing="2">
        <tbody>
            <tr> 
            <td> 
            <img
                src="../images/MC_err.png"
                alt="Result of propagated error after MC"
                width="100%"
                title="Result of propagated error after MC">
                <p style="color:rgb(51,51,153);text-align:center"> 
                <i>MC propagated error after 10, 100, and 500 iterations</i>
                </p>
            </td>
            </tr>
        </tbody>
        </table>
        <br>
        <br>
        <br>
        <br><br>
        <p style="font-size:10px">
        \(^{*}\) If \(f\) is approximately affine in the region about the mean
        of the distribution<br><br>
        \(^{**}\) Aniano, G., Draine, B. T., Gordon, K. D., and Sandstrom, K.,
        “Common-Resolution Convolution Kernels for Space- and Ground-Based
        Telescopes”, <i>Publications of the Astronomical Society of the
        Pacific</i>, vol. 123, no. 908, p. 1218, 2011. doi:10.1086/662219.<br>
        <br>
        \(^{***}\) Randolf Klein 2021 Res. Notes AAS 5 39<br>
        
      </div>
            
      <br>
      
      <hr>
      <p style="text-align:center">
      <a href="../index.html"><b>Home</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="../about.html"><b>About</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="../research.html"><b>Research</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="../software.html"><b>Software</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="../blog.html"><b>Blog</b></a>
      <hr>
      <br><br>
    </body> 
</html>
